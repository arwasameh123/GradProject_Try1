{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13446390,"sourceType":"datasetVersion","datasetId":8535130},{"sourceId":13446483,"sourceType":"datasetVersion","datasetId":8535193}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ==== Kaggle setup (run once per session) ====\n# Make sure Internet is ON in Kaggle notebook settings.\n\n# System deps: Poppler (for pdf2image) + Tesseract with Arabic data\n!apt-get -y update >/dev/null\n!apt-get -y install -qq poppler-utils tesseract-ocr tesseract-ocr-ara >/dev/null\n\n# Python libs\n!pip -q install pdfplumber PyMuPDF arabic-reshaper python-bidi pytesseract pdf2image","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-20T19:07:18.948975Z","iopub.execute_input":"2025-10-20T19:07:18.949246Z","iopub.status.idle":"2025-10-20T19:07:44.371673Z","shell.execute_reply.started":"2025-10-20T19:07:18.949221Z","shell.execute_reply":"2025-10-20T19:07:44.370670Z"}},"outputs":[{"name":"stdout","text":"W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ===== Robust Arabic PDF extractor (ensemble, batch, headers learned) =====\n# What it does\n# - Processes all PDFs in INPUT_DIR (and you can attach multiple datasets)\n# - For each page, tries pdfplumber → PyMuPDF(text) → PyMuPDF(blocks), then OCR if needed\n# - Picks best candidate by Arabic quality\n# - Learns repeating headers/footers (per document) and strips them\n# - Applies gentle OCR fixes (percent signs, digits)\n# - Saves per-PDF:\n#       /kaggle/working/outputs/<pdf-stem>/\n#           ├─ pages.jsonl              (one JSON per page: page, source, text)\n#           ├─ output_logical.txt       (NLP-friendly with [[PAGE | SOURCE]] markers)\n#           └─ output_readable.txt      (reshaped RTL for human reading)\n# - Also writes:\n#       /kaggle/working/outputs/corpus_pages.jsonl   (all docs aggregated)\n\nimport os, re, json, unicodedata, glob, shutil\nimport pdfplumber, fitz\nimport pytesseract\nfrom pdf2image import convert_from_path\nimport arabic_reshaper\nfrom bidi.algorithm import get_display\nfrom pathlib import Path\nfrom collections import Counter\n\n# ========= Configure your paths here =========\n# Attach your dataset(s) in the right panel → Add data. Then set INPUT_DIR to that path.\nINPUT_DIR  = \"/kaggle/input/tax-data\"   # <-- change to your dataset folder\nOUTPUT_DIR = \"/kaggle/working/outputs\"\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# ========= OCR and extraction settings =========\nOCR_LANG        = \"ara\"\nOCR_CONFIG      = \"--oem 3 --psm 4\"     # try --psm 4 for multi-column pages\nMIN_LEN         = 30                    # below this we assume page failed\nAR_RATIO_THRESH = 0.15                  # minimal Arabic ratio to accept non-OCR text\nUSE_PDF2IMAGE   = True                  # If apt-get is not allowed, set to False (uses PyMuPDF rasterization)\n\n# ========= Arabic helpers =========\ndef has_arabic_ratio(s: str, thresh: float = AR_RATIO_THRESH) -> bool:\n    if not s:\n        return False\n    ar = re.findall(r'[\\u0600-\\u06FF]', s)\n    return (len(ar) / max(1, len(s))) >= thresh\n\ndef normalize_for_search(s: str) -> str:\n    s = unicodedata.normalize(\"NFKC\", s)\n    s = re.sub(r'[ـ]+', '', s)\n    s = re.sub(r'[ \\t]+', ' ', s)\n    s = re.sub(r'[\\r\\n]+', '\\n', s)\n    s = s.translate(str.maketrans('٠١٢٣٤٥٦٧٨٩', '0123456789'))\n    return s\n\ndef make_readable_arabic(s: str) -> str:\n    reshaped = arabic_reshaper.reshape(s)\n    return get_display(reshaped)\n\ndef fix_common_ocr(text: str) -> str:\n    # unify percent variants and tidy spaces: \"60 %\" -> \"60%\"\n    text = text.replace(\"٪\",\"%\").replace(\"﹪\",\"%\")\n    text = re.sub(r\"(\\d)\\s*%\", r\"\\1%\", text)          # 60 % -> 60%\n    text = re.sub(r\"(?<=\\d)\\s+(?=\\d)\", \"\", text)      # 6 0 -> 60\n    # Domain-specific safe fix: misread \"60%\" as \"9060\" before \"من مبلغ/قيمة/الضريبة\"\n    text = re.sub(r\"\\b90?60\\b(?=\\s+(?:من|عن)\\s+(?:مبلغ|قيمة|الضريبة))\", \"60%\", text)\n    # Harmonize digits\n    text = text.translate(str.maketrans('٠١٢٣٤٥٦٧٨٩','0123456789'))\n    return text\n\ndef score_arabic_quality(s: str) -> float:\n    \"\"\"Simple score: length-weighted Arabic ratio.\"\"\"\n    if not s: return 0.0\n    return (len(s) / 1000.0) * (1.0 if has_arabic_ratio(s) else 0.0)\n\n# ========= Extraction per page (multi-method) =========\ndef extract_page_pdfplumber(pg) -> str:\n    try:\n        t = pg.extract_text() or \"\"\n        return t.strip()\n    except Exception:\n        return \"\"\n\ndef extract_page_pymupdf_text(p: fitz.Page) -> str:\n    try:\n        t = p.get_text(\"text\") or \"\"\n        return t.strip()\n    except Exception:\n        return \"\"\n\ndef extract_page_pymupdf_blocks(p: fitz.Page) -> str:\n    # Sort blocks top-to-bottom then left-to-right; join lines\n    try:\n        blocks = p.get_text(\"blocks\") or []\n        blocks = sorted(blocks, key=lambda b: (round(b[1],2), round(b[0],2)))\n        lines = []\n        for b in blocks:\n            txt = (b[4] or \"\").strip()\n            if txt:\n                lines.append(txt)\n        return \"\\n\".join(lines).strip()\n    except Exception:\n        return \"\"\n\ndef rasterize_pymupdf(doc: fitz.Document, i: int, dpi: int = 300):\n    \"\"\"Rasterize page i with PyMuPDF; returns PIL Image.\"\"\"\n    # PyMuPDF returns pixmaps; convert to PIL\n    page = doc[i]\n    zoom = dpi / 72.0\n    mat = fitz.Matrix(zoom, zoom)\n    pix = page.get_pixmap(matrix=mat, alpha=False)\n    try:\n        from PIL import Image\n        mode = \"RGB\" if pix.n < 4 else \"RGBA\"\n        im = Image.frombytes(mode, [pix.width, pix.height], pix.samples)\n        return im\n    except Exception:\n        return None\n\ndef ocr_page_image(images_or_doc, idx, via_pdf2image=True) -> str:\n    if via_pdf2image:\n        im = images_or_doc[idx]\n    else:\n        im = rasterize_pymupdf(images_or_doc, idx, dpi=300)\n        if im is None:\n            return \"\"\n    txt = pytesseract.image_to_string(im, lang=OCR_LANG, config=OCR_CONFIG) or \"\"\n    return txt.strip()\n\n# ========= Learn & strip headers/footers per document =========\ndef learn_repeating_edge_lines(page_texts, k=2, freq_threshold=0.5):\n    \"\"\"\n    Collect first/last k non-empty lines per page, count frequency,\n    and return sets of lines that appear in >= freq_threshold of pages.\n    \"\"\"\n    first_counts, last_counts = Counter(), Counter()\n    n_pages = len(page_texts)\n\n    for t in page_texts:\n        lines = [ln.strip() for ln in t.splitlines() if ln.strip()]\n        if not lines: \n            continue\n        first_counts.update(lines[:min(k, len(lines))])\n        last_counts.update(lines[-min(k, len(lines)):])\n\n    first_common = {ln for ln, c in first_counts.items() if c >= freq_threshold * n_pages}\n    last_common  = {ln for ln, c in last_counts.items()  if c >= freq_threshold * n_pages}\n    return first_common, last_common\n\ndef strip_learned_headers_footers(text, first_common, last_common):\n    out = []\n    for ln in text.splitlines():\n        s = ln.strip()\n        if s in first_common or s in last_common:\n            continue\n        out.append(ln)\n    return \"\\n\".join(out)\n\n# ========= Process a single PDF =========\ndef process_pdf(pdf_path: str, out_dir_root: str):\n    stem = Path(pdf_path).stem\n    out_dir = os.path.join(out_dir_root, stem)\n    os.makedirs(out_dir, exist_ok=True)\n\n    # Open once for both engines\n    doc_pl = pdfplumber.open(pdf_path)\n    doc_fz = fitz.open(pdf_path)\n\n    # Render images lazily only if needed\n    images = None\n\n    records_raw = []  # before header/footer stripping\n    max_pages = max(len(doc_pl.pages), doc_fz.page_count)\n\n    for i in range(max_pages):\n        page_num = i + 1\n        cand = []  # [(source, text, score)]\n\n        # pdfplumber\n        if i < len(doc_pl.pages):\n            t1 = extract_page_pdfplumber(doc_pl.pages[i])\n            cand.append((\"pdfplumber\", t1, score_arabic_quality(t1)))\n\n        # PyMuPDF text & blocks\n        if i < doc_fz.page_count:\n            p = doc_fz[i]\n            t2 = extract_page_pymupdf_text(p)\n            cand.append((\"pymupdf_text\", t2, score_arabic_quality(t2)))\n            t3 = extract_page_pymupdf_blocks(p)\n            cand.append((\"pymupdf_blocks\", t3, score_arabic_quality(t3)))\n\n        # Choose best non-OCR candidate\n        cand.sort(key=lambda x: x[2], reverse=True)\n        best_src, best_txt, best_score = (cand[0] if cand else (\"\", \"\", 0.0))\n\n        # If too short or poor Arabic ratio, OCR\n        if (len(best_txt.strip()) < MIN_LEN) or (not has_arabic_ratio(best_txt, AR_RATIO_THRESH)):\n            if USE_PDF2IMAGE:\n                if images is None:\n                    images = convert_from_path(pdf_path, dpi=300)\n                best_txt = ocr_page_image(images, i, via_pdf2image=True)\n            else:\n                best_txt = ocr_page_image(doc_fz, i, via_pdf2image=False)\n            best_src = \"ocr\"\n\n        # Cleanup OCR/common artifacts\n        best_txt = fix_common_ocr(best_txt)\n\n        records_raw.append({\"page\": page_num, \"source\": best_src, \"text\": best_txt})\n\n    # Learn headers/footers and strip\n    first_common, last_common = learn_repeating_edge_lines([r[\"text\"] for r in records_raw])\n    records = []\n    for r in records_raw:\n        cleaned = strip_learned_headers_footers(r[\"text\"], first_common, last_common).strip()\n        records.append({\"page\": r[\"page\"], \"source\": r[\"source\"], \"text\": cleaned})\n\n    # Save per-pdf JSONL\n    pages_jsonl = os.path.join(out_dir, \"pages.jsonl\")\n    with open(pages_jsonl, \"w\", encoding=\"utf-8\") as f:\n        for r in records:\n            f.write(json.dumps({\"doc\": os.path.basename(pdf_path), **r}, ensure_ascii=False) + \"\\n\")\n\n    # Save logical & readable\n    logical_path  = os.path.join(out_dir, \"output_logical.txt\")\n    readable_path = os.path.join(out_dir, \"output_readable.txt\")\n\n    logical_text = \"\\n\\n\".join(f\"[[PAGE {r['page']} | {r['source'].upper()}]]\\n{r['text']}\" for r in records).strip()\n    with open(logical_path, \"w\", encoding=\"utf-8\") as f:\n        f.write(logical_text)\n\n    readable_text = make_readable_arabic(logical_text)\n    with open(readable_path, \"w\", encoding=\"utf-8\") as f:\n        f.write(readable_text)\n\n    # Close docs\n    doc_pl.close()\n    doc_fz.close()\n\n    return out_dir, records\n\n# ========= Batch over a folder =========\ndef process_folder(input_dir: str, output_dir: str):\n    pdfs = sorted(glob.glob(os.path.join(input_dir, \"*.pdf\")))\n    if not pdfs:\n        print(f\"⚠️ No PDFs found in: {input_dir}\")\n        return\n\n    corpus_path = os.path.join(output_dir, \"corpus_pages.jsonl\")\n    with open(corpus_path, \"w\", encoding=\"utf-8\") as corpus:\n        for pdf in pdfs:\n            print(f\"📄 Processing: {os.path.basename(pdf)}\")\n            out_dir, recs = process_pdf(pdf, output_dir)\n            for r in recs:\n                corpus.write(json.dumps({\"doc\": os.path.basename(pdf), **r}, ensure_ascii=False) + \"\\n\")\n            print(f\"   ✅ Saved → {out_dir}\")\n\n    print(\"\\n🎉 Done. Corpus JSONL:\", corpus_path)\n\n# ========= RUN =========\nprocess_folder(INPUT_DIR, OUTPUT_DIR)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T19:07:44.373495Z","iopub.execute_input":"2025-10-20T19:07:44.373730Z","iopub.status.idle":"2025-10-20T19:21:22.050580Z","shell.execute_reply.started":"2025-10-20T19:07:44.373691Z","shell.execute_reply":"2025-10-20T19:21:22.049737Z"}},"outputs":[{"name":"stdout","text":"📄 Processing: -    -  .pdf\n   ✅ Saved → /kaggle/working/outputs/-    -  \n📄 Processing: 2017 .pdf\n   ✅ Saved → /kaggle/working/outputs/2017 \n📄 Processing: 2020_0.pdf\n   ✅ Saved → /kaggle/working/outputs/2020_0\n\n🎉 Done. Corpus JSONL: /kaggle/working/outputs/corpus_pages.jsonl\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ==== FIXED unit builder: raw-span extraction + broader headers (circulars) ====\n# Inputs:  /kaggle/working/outputs/corpus_pages.jsonl\n# Outputs: /kaggle/working/outputs/units.jsonl, per-doc units.jsonl, units_summary.csv\n\nimport os, re, json, csv, unicodedata\nfrom pathlib import Path\nfrom collections import defaultdict\n\nOUTPUT_ROOT = \"/kaggle/working/outputs\"\nCORPUS_JSONL = f\"{OUTPUT_ROOT}/corpus_pages.jsonl\"\nUNITS_JSONL  = f\"{OUTPUT_ROOT}/units.jsonl\"\nUNITS_CSV    = f\"{OUTPUT_ROOT}/units_summary.csv\"\nassert os.path.exists(CORPUS_JSONL), f\"Missing {CORPUS_JSONL}. Run the extractor first.\"\n\ndef normalize_for_search(s: str) -> str:\n    s = unicodedata.normalize(\"NFKC\", s)\n    s = re.sub(r'[ـ]+', '', s)\n    s = re.sub(r'[ \\t]+', ' ', s)\n    s = re.sub(r'[\\r\\n]+', '\\n', s)\n    s = s.translate(str.maketrans('٠١٢٣٤٥٦٧٨٩','0123456789'))\n    s = s.replace(\"أ\",\"ا\").replace(\"إ\",\"ا\").replace(\"آ\",\"ا\")\n    return s\n\n# --- Regex on RAW text (no normalization in spans) ---\nART_RE_RAW = re.compile(\n    r'(?m)^(?:المادة|مادة)\\s*[\\(]?\\s*(\\d+)\\s*[\\)]?\\s*(.*?)(?=^(?:المادة|مادة)\\s*[\\(]?\\d+|\\Z)',\n    flags=re.DOTALL\n)\n\n# Broader family of admin headers (very common in ETA PDFs)\nCIRC_FAMILY = r'(?:تعليمات(?:\\s+تنفيذية)?|منشور(?:\\s+عام)?|كتاب\\s+دوري|قرار|بيان)'\nCIRC_RE_RAW = re.compile(\n    rf'(?ms)^\\s*{CIRC_FAMILY}\\s*(?:رقم\\s*\\(([^)]+)\\))?\\s*(?:لسنة\\s*([0-9]{{3,4}}))?.*?(?=^\\s*{CIRC_FAMILY}\\b|^\\s*(?:المادة|مادة)\\b|\\Z)'\n)\n\n# Some docs have headings like: \"تعليمات رقم 12 لسنة 2020 بشأن ...\" without parentheses\nCIRC_ALT_RE_RAW = re.compile(\n    rf'(?ms)^\\s*{CIRC_FAMILY}\\s*(?:رقم\\s*([0-9]+))?\\s*(?:لسنة\\s*([0-9]{{3,4}}))?.*?(?=^\\s*{CIRC_FAMILY}\\b|^\\s*(?:المادة|مادة)\\b|\\Z)'\n)\n\ndef guess_doc_type_raw(raw: str) -> str:\n    art_hits  = len(re.findall(r'(?m)^(?:المادة|مادة)\\s*\\(?\\d+', raw))\n    circ_hits = len(re.findall(rf'(?m)^\\s*{CIRC_FAMILY}', raw))\n    if art_hits >= 5 and art_hits >= circ_hits * 2: return \"law_or_reg\"\n    if circ_hits >= 1 and circ_hits >= art_hits:    return \"circulars\"\n    return \"unknown\"\n\ndef join_pages_to_text(pages):\n    if not pages: return \"\"\n    pages = sorted(set(pages))\n    if len(pages) == 1: return f\"{pages[0]}\"\n    ranges, start, prev = [], pages[0], pages[0]\n    for p in pages[1:]:\n        if p == prev + 1:\n            prev = p\n        else:\n            ranges.append((start, prev)); start = prev = p\n    ranges.append((start, prev))\n    return \", \".join(f\"{a}\" if a==b else f\"{a}-{b}\" for a,b in ranges)\n\ndef pages_covered(span_start, span_end, page_markers):\n    # page_markers: list[(pos_after_marker_line, page_number)]\n    pages = []\n    for pos, pg in page_markers:\n        if span_start <= pos < span_end:\n            pages.append(pg)\n    if not pages and page_markers:\n        # include nearest previous page\n        prev = None\n        for pos, pg in page_markers:\n            if pos <= span_start: prev = pg\n            else: break\n        if prev is not None: pages = [prev]\n    return sorted(set(pages))\n\ndef safe_stem(name: str) -> str:\n    # sanitize folder names like \"-    -  .pdf\"\n    stem = Path(name).stem.strip()\n    stem = re.sub(r'[\\\\/:*?\"<>|]+', '_', stem)   # windows-forbidden\n    stem = re.sub(r'\\s+', ' ', stem).strip()\n    return stem or \"doc\"\n\n# ---- Load corpus ----\ndocs = defaultdict(list)\nwith open(CORPUS_JSONL, \"r\", encoding=\"utf-8\") as f:\n    for line in f:\n        if not line.strip(): continue\n        rec = json.loads(line)\n        docs[rec[\"doc\"]].append((rec.get(\"page\"), rec.get(\"text\",\"\")))\n\n# ---- Build units per doc ----\nall_units = []\nsummary_rows = []\n\nfor doc, page_blocks in docs.items():\n    page_blocks = sorted(page_blocks, key=lambda x: (x[0] if x[0] else 0))\n    stem = safe_stem(doc)\n    per_doc_dir = f\"{OUTPUT_ROOT}/{stem}\"\n    os.makedirs(per_doc_dir, exist_ok=True)\n\n    # Build RAW full text with page markers for span→page mapping\n    combined, marker_positions, pos = [], [], 0\n    for pg, txt in page_blocks:\n        marker = f\"[[PAGE {pg}]]\\n\"\n        combined.append(marker); pos += len(marker)\n        marker_positions.append((pos, pg))\n        combined.append(txt or \"\"); pos += len(txt or \"\")\n        combined.append(\"\\n\\n\"); pos += 2\n    raw_text = \"\".join(combined)\n\n    dtype = guess_doc_type_raw(raw_text)\n    units = []\n\n    if dtype == \"law_or_reg\":\n        for m in ART_RE_RAW.finditer(raw_text):\n            art_no = m.group(1)\n            s, e = m.span()\n            chunk_raw = raw_text[s:e].strip()\n            pg_list   = pages_covered(s, e, marker_positions)\n            units.append({\n                \"doc_id\": doc,\n                \"unit_type\": \"article\",\n                \"unit_id\": f\"المادة {art_no}\",\n                \"text\": chunk_raw,\n                \"search_text\": normalize_for_search(chunk_raw),\n                \"pages\": pg_list,\n                \"pages_human\": join_pages_to_text(pg_list),\n                \"effective_from\": None, \"effective_to\": None, \"version_note\": None\n            })\n\n    elif dtype == \"circulars\":\n        # Try strict pattern first, then relaxed\n        matches = list(CIRC_RE_RAW.finditer(raw_text))\n        if not matches:\n            matches = list(CIRC_ALT_RE_RAW.finditer(raw_text))\n        for m in matches:\n            num = (m.group(1) or \"\").strip()\n            yr  = (m.group(2) or \"\").strip()\n            s, e = m.span()\n            chunk_raw = raw_text[s:e].strip()\n            pg_list   = pages_covered(s, e, marker_positions)\n            unit_id = f\"{'تعليمات/منشور/كتاب/قرار'}\"\n            if num and yr:\n                unit_id = f\"{unit_id} رقم {num} لسنة {yr}\"\n            elif num:\n                unit_id = f\"{unit_id} رقم {num}\"\n            units.append({\n                \"doc_id\": doc,\n                \"unit_type\": \"circular\",\n                \"unit_id\": unit_id,\n                \"text\": chunk_raw,\n                \"search_text\": normalize_for_search(chunk_raw),\n                \"pages\": pg_list,\n                \"pages_human\": join_pages_to_text(pg_list),\n                \"effective_from\": None, \"effective_to\": None, \"version_note\": None\n            })\n\n    # Fallback if nothing matched or dtype unknown: big sections\n    if not units:\n        # Split on large gaps or major headings\n        blocks = re.split(r\"\\n{3,}\", raw_text)\n        for idx, blk in enumerate(blocks, 1):\n            b = blk.strip()\n            if len(b) < 200: \n                continue\n            s = raw_text.find(blk)\n            e = s + len(blk)\n            pg_list = pages_covered(s, e, marker_positions)\n            units.append({\n                \"doc_id\": doc, \"unit_type\": \"section\",\n                \"unit_id\": f\"section-{idx}\",\n                \"text\": b, \"search_text\": normalize_for_search(b),\n                \"pages\": pg_list, \"pages_human\": join_pages_to_text(pg_list),\n                \"effective_from\": None, \"effective_to\": None, \"version_note\": None\n            })\n\n    # Save per-doc units\n    per_doc_units = f\"{per_doc_dir}/units.jsonl\"\n    with open(per_doc_units, \"w\", encoding=\"utf-8\") as out:\n        for u in units:\n            out.write(json.dumps(u, ensure_ascii=False) + \"\\n\")\n\n    all_units.extend(units)\n    summary_rows.append({\"doc\": doc, \"type\": dtype, \"pages\": len(page_blocks), \"units\": len(units)})\n\n# ---- Write global files ----\nwith open(UNITS_JSONL, \"w\", encoding=\"utf-8\") as f:\n    for u in all_units:\n        f.write(json.dumps(u, ensure_ascii=False) + \"\\n\")\n\nwith open(UNITS_CSV, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n    w = csv.DictWriter(f, fieldnames=[\"doc\",\"type\",\"pages\",\"units\"])\n    w.writeheader()\n    for row in summary_rows:\n        w.writerow(row)\n\nprint(f\"✅ Rebuilt units. Total = {len(all_units)}\")\nprint(f\"• All units  : {UNITS_JSONL}\")\nprint(f\"• Summary CSV: {UNITS_CSV}\")\nprint(\"• Per-doc units under /kaggle/working/outputs/<doc-stem>/units.jsonl\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T19:36:19.060264Z","iopub.execute_input":"2025-10-20T19:36:19.064223Z","iopub.status.idle":"2025-10-20T19:36:19.141097Z","shell.execute_reply.started":"2025-10-20T19:36:19.064155Z","shell.execute_reply":"2025-10-20T19:36:19.140043Z"}},"outputs":[{"name":"stdout","text":"✅ Rebuilt units. Total = 24\n• All units  : /kaggle/working/outputs/units.jsonl\n• Summary CSV: /kaggle/working/outputs/units_summary.csv\n• Per-doc units under /kaggle/working/outputs/<doc-stem>/units.jsonl\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import os, json, csv, glob, random, pandas as pd\nfrom pathlib import Path\n\nROOT = \"/kaggle/working/outputs\"\nUNITS_JSONL = f\"{ROOT}/units.jsonl\"\nSUMMARY_CSV = f\"{ROOT}/units_summary.csv\"\n\nassert os.path.exists(SUMMARY_CSV), \"units_summary.csv not found\"\nassert os.path.exists(UNITS_JSONL), \"units.jsonl not found\"\n\n# 1) Overview\nsummary = pd.read_csv(SUMMARY_CSV)\ndisplay(summary)\n\nprint(\"\\nDocs with 0 units (should be none):\")\nprint(summary[summary[\"units\"] == 0])\n\n# 2) Load all units\nunits = []\nwith open(UNITS_JSONL, \"r\", encoding=\"utf-8\") as f:\n    for line in f:\n        if line.strip():\n            units.append(json.loads(line))\nprint(f\"\\nTotal units: {len(units)}\")\n\ndf = pd.DataFrame(units)\n\n# 3) Basic health checks\nprint(\"\\nUnit types distribution:\")\nprint(df[\"unit_type\"].value_counts())\n\nprint(\"\\nEmpty or too-short texts (<120 chars):\")\nshort = df[df[\"text\"].str.len().fillna(0) < 120]\nprint(short[[\"doc_id\",\"unit_type\",\"unit_id\"]].head(10))\n\nprint(\"\\nUnits missing pages:\")\nmissing_pages = df[df[\"pages\"].apply(lambda x: not x)]\nprint(missing_pages[[\"doc_id\",\"unit_type\",\"unit_id\"]].head(10))\n\n# 4) Per-doc spot checks (3 samples each)\nfor doc, sub in df.groupby(\"doc_id\"):\n    print(f\"\\n===== {doc} | {len(sub)} units =====\")\n    sample = sub.sample(min(3, len(sub)), random_state=42)\n    for _, r in sample.iterrows():\n        print(f\"- [{r['unit_type']}] {r['unit_id']}  | pages: {r['pages']}  | text_len: {len(r['text'])}\")\n\n# 5) Ensure per-doc files exist\nstems = set(Path(p).stem for p in glob.glob(f\"{ROOT}/*/*.pdf\"))  # not used (no PDFs here), so check by folder name\nperdoc_units = glob.glob(f\"{ROOT}/*/units.jsonl\")\nprint(f\"\\nPer-doc units files found: {len(perdoc_units)}\")\nfor p in perdoc_units[:10]:\n    print(\"•\", p)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T19:36:21.998981Z","iopub.execute_input":"2025-10-20T19:36:21.999277Z","iopub.status.idle":"2025-10-20T19:36:22.054506Z","shell.execute_reply.started":"2025-10-20T19:36:21.999258Z","shell.execute_reply":"2025-10-20T19:36:22.053548Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"            doc       type  pages  units\n0  -    -  .pdf  circulars    130      2\n1     2017 .pdf  circulars     26      1\n2    2020_0.pdf  circulars     41     21","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>doc</th>\n      <th>type</th>\n      <th>pages</th>\n      <th>units</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-    -  .pdf</td>\n      <td>circulars</td>\n      <td>130</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2017 .pdf</td>\n      <td>circulars</td>\n      <td>26</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2020_0.pdf</td>\n      <td>circulars</td>\n      <td>41</td>\n      <td>21</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"\nDocs with 0 units (should be none):\nEmpty DataFrame\nColumns: [doc, type, pages, units]\nIndex: []\n\nTotal units: 24\n\nUnit types distribution:\nunit_type\ncircular    24\nName: count, dtype: int64\n\nEmpty or too-short texts (<120 chars):\n        doc_id unit_type                  unit_id\n3   2020_0.pdf  circular  تعليمات/منشور/كتاب/قرار\n11  2020_0.pdf  circular  تعليمات/منشور/كتاب/قرار\n17  2020_0.pdf  circular  تعليمات/منشور/كتاب/قرار\n\nUnits missing pages:\nEmpty DataFrame\nColumns: [doc_id, unit_type, unit_id]\nIndex: []\n\n===== -    -  .pdf | 2 units =====\n- [circular] تعليمات/منشور/كتاب/قرار  | pages: [88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108]  | text_len: 16896\n- [circular] تعليمات/منشور/كتاب/قرار  | pages: [59, 60, 61, 62, 63, 64, 65, 66, 67]  | text_len: 11075\n\n===== 2017 .pdf | 1 units =====\n- [circular] تعليمات/منشور/كتاب/قرار  | pages: [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]  | text_len: 31960\n\n===== 2020_0.pdf | 21 units =====\n- [circular] تعليمات/منشور/كتاب/قرار  | pages: [3]  | text_len: 59\n- [circular] تعليمات/منشور/كتاب/قرار  | pages: [32, 33, 34, 35, 36, 37, 38]  | text_len: 8461\n- [circular] تعليمات/منشور/كتاب/قرار  | pages: [27]  | text_len: 580\n\nPer-doc units files found: 5\n• /kaggle/working/outputs/- -/units.jsonl\n• /kaggle/working/outputs/2017 /units.jsonl\n• /kaggle/working/outputs/-    -  /units.jsonl\n• /kaggle/working/outputs/2017/units.jsonl\n• /kaggle/working/outputs/2020_0/units.jsonl\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# ==== Load & Preview Your Legal Units Dataset ====\nimport pandas as pd\nimport json\n\n# Path to your structured dataset\nDATA_PATH = \"/kaggle/working/outputs/units.jsonl\"\n\n# Load JSONL into a DataFrame\ndf = pd.read_json(DATA_PATH, lines=True)\n\n# Basic info\nprint(f\"✅ Loaded {len(df):,} units from {DATA_PATH}\")\nprint(\"Columns:\", list(df.columns))\n\n# Show basic stats\nprint(\"\\n📊 Document distribution:\")\ndisplay(df.groupby(\"doc_id\")[\"unit_id\"].count().reset_index(name=\"num_units\"))\n\nprint(\"\\n🧩 Unit types distribution:\")\ndisplay(df[\"unit_type\"].value_counts())\n\n# Show a random sample of 3 units\nprint(\"\\n🔎 Sample of extracted units:\")\ndisplay(df[[\"doc_id\", \"unit_type\", \"unit_id\", \"pages_human\", \"text\"]].sample(3, random_state=42))\n\n# Optional: view average text length per document\ndf[\"text_len\"] = df[\"text\"].apply(lambda x: len(x or \"\"))\navg_len = df.groupby(\"doc_id\")[\"text_len\"].mean().reset_index()\navg_len.columns = [\"doc_id\", \"avg_text_len\"]\nprint(\"\\n📏 Average unit text length by document:\")\ndisplay(avg_len)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T19:40:39.464605Z","iopub.execute_input":"2025-10-20T19:40:39.464947Z","iopub.status.idle":"2025-10-20T19:40:39.523659Z","shell.execute_reply.started":"2025-10-20T19:40:39.464924Z","shell.execute_reply":"2025-10-20T19:40:39.522853Z"}},"outputs":[{"name":"stdout","text":"✅ Loaded 24 units from /kaggle/working/outputs/units.jsonl\nColumns: ['doc_id', 'unit_type', 'unit_id', 'text', 'search_text', 'pages', 'pages_human', 'effective_from', 'effective_to', 'version_note']\n\n📊 Document distribution:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"         doc_id  num_units\n0  -    -  .pdf          2\n1     2017 .pdf          1\n2    2020_0.pdf         21","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>doc_id</th>\n      <th>num_units</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-    -  .pdf</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2017 .pdf</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2020_0.pdf</td>\n      <td>21</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"\n🧩 Unit types distribution:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"unit_type\ncircular    24\nName: count, dtype: int64"},"metadata":{}},{"name":"stdout","text":"\n🔎 Sample of extracted units:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"          doc_id unit_type                  unit_id pages_human  \\\n8     2020_0.pdf  circular  تعليمات/منشور/كتاب/قرار          12   \n16    2020_0.pdf  circular  تعليمات/منشور/كتاب/قرار          26   \n0   -    -  .pdf  circular  تعليمات/منشور/كتاب/قرار       59-67   \n\n                                                 text  \n8   تعليمات\\nرقم ر / )لسنة ‎0111‏\\n\\nفي إطار السعي...  \n16  تعليمات ‏\\nرقم ( كدب ) لسئة ‎3101‏\\n‏بشاأن\\n‏ا...  \n0   بيانات الفواتير الضريبية الصادرة من المسجلين\\n...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>doc_id</th>\n      <th>unit_type</th>\n      <th>unit_id</th>\n      <th>pages_human</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>8</th>\n      <td>2020_0.pdf</td>\n      <td>circular</td>\n      <td>تعليمات/منشور/كتاب/قرار</td>\n      <td>12</td>\n      <td>تعليمات\\nرقم ر / )لسنة ‎0111‏\\n\\nفي إطار السعي...</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>2020_0.pdf</td>\n      <td>circular</td>\n      <td>تعليمات/منشور/كتاب/قرار</td>\n      <td>26</td>\n      <td>تعليمات ‏\\nرقم ( كدب ) لسئة ‎3101‏\\n‏بشاأن\\n‏ا...</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>-    -  .pdf</td>\n      <td>circular</td>\n      <td>تعليمات/منشور/كتاب/قرار</td>\n      <td>59-67</td>\n      <td>بيانات الفواتير الضريبية الصادرة من المسجلين\\n...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"\n📏 Average unit text length by document:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"         doc_id  avg_text_len\n0  -    -  .pdf  13985.500000\n1     2017 .pdf  31960.000000\n2    2020_0.pdf   2213.142857","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>doc_id</th>\n      <th>avg_text_len</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-    -  .pdf</td>\n      <td>13985.500000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2017 .pdf</td>\n      <td>31960.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2020_0.pdf</td>\n      <td>2213.142857</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"import re\nbad = df[df[\"text\"].str.contains(r'[A-Za-z0-9]{20,}', na=False)]\nprint(f\"Gibberish-like segments: {len(bad)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T19:42:48.395344Z","iopub.execute_input":"2025-10-20T19:42:48.395631Z","iopub.status.idle":"2025-10-20T19:42:48.404654Z","shell.execute_reply.started":"2025-10-20T19:42:48.395612Z","shell.execute_reply":"2025-10-20T19:42:48.403740Z"}},"outputs":[{"name":"stdout","text":"Gibberish-like segments: 1\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}